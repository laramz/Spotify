\newpage
# Popularity Prediction -- Application of Statistical Methods

In the last part, you got a feel for the data set. You now know which variables the data set contains and a few characteristic properties of it. But so far, we have only visualized the data set. This section goes a step further and uses statistical methods to predict a song's popularity as accurately as possible.

To compare your models, you will use a uniform metric to check the predictions for accuracy. In your case, this is the Root Mean Square Error (RMSE), i.e., the root of the average squared difference between the predicted ($\hat{y}_i$) and the actual value ($y_i$) :

$$ RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N}{(\hat{y}_i-y_i)^2}} $$

The closer the $RMSE$ is to 0, the better your model predicts prices. Therefore, in the following section, your goal is to reduce the $RMSE$ of your various models as much as possible through continuous improvements.
 
We use the Spotify data set, which contains features about the songs as well as their popularity. When we train our model with the data, we need to estimate how well our model predicts popularity values for songs it has never seen before. Therefore we split our data set into two parts - a train and a test set.
 
Here is a brief description of why you need each data set:
 
* *train.csv (70 %)*: You use this training data set to train your model. The model can learn the relationships between the variables based on the training data set containing both the variables needed to predict the popularity and the actual popularity variable.
 
* *test.csv (30 %)*: With this test data set, you can test how well your model predicts the popularity using data it has not seen before. This prediction will help you, for example, to recognize under- or overfitting.
 
 
:::: {.tips .r data-latex="r"}
There are multiple ways to split/partition the data set in R. You could, e.g., use `createDataPartition()` function from [{caret}](https://cran.r-project.org/package=caret), `sample.split()` function from [{caTools}](https://cran.r-project.org/package=caTools) or  — perhaps most intuitively  — `split_train_test()` function from [{healthcareai}](https://CRAN.R-project.org/package=healthcareai). 

Make sure, however, to include the `set.seed()` function in your R code before randomly dividing the data set into train/test parts. `set.seed()` function allows you to reproduce randomness each time you re-run your code. That means that you always get the same (random) result -- a vital aspect of reproducibility within the scientific method. 
::::

:::: {.tipsp .python data-latex="p"}
The sklearn library has a function that you can use to split the data set, correspondingly 
`from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df, df.popularity, test_size=0.3)`
::::


## Examine the Correlation Between the Variables (train)

How are the individual variables related to each other? In other words, to what extent do the variables in the data set correlate with one another? Finding this out is vital for deciding which model specification to use later. A good place to start is to create a correlation matrix:

:::: {.tips .r data-latex="r"}
Use the `cor()` function from the {base} R to create a correlation matrix. Select all numerical variables in your data set with the help of `sapply()` or {dplyr}’s `select()` and create a correlation matrix.
 
Alternatively, use the `ggcorrplot()` from the same-name [{ggcorrplot}](https://cran.r-project.org/package=ggcorrplot) package.
 
Check out [this article](http://jamesmarquezportfolio.com/correlation_matrices_in_r.html) by James Marquez if you're curious about other ways to create a correlation plot in R.
::::
 
 
:::: {.tipsp .python data-latex="p"}
You can show the correlations between the features by using the `.corr()` function on a data frame. You can additionally get a colorized version by using `df.corr().style.background_gradient()`
::::

Which of your examined variables correlates most with popularity and which seems to be more independent from popularity? You now have a first impression of which variables could be vital for your model. So let’s get to your first song popularity prediction model!

## First Predictions with Simple Regression Models (train)

Now you can make use of your statistical knowledge. You will need a method with which you can predict the popularity of a Spotify song.
 
Have you ever heard of linear regression? Now you can use your statistics skills. First, set up a model with popularity as the dependent variable. In the previous exercise, you examined different variables. Now choose the variable with the highest correlation to popularity and use that as the only independent variable.
 
For example, your first regression model could look like this:

$$ popularity = \beta_0 + \beta_1 \cdot danceability + \epsilon $$
 
 
:::: {.tips .r data-latex="r"}
In R, you can implement a simple linear regression with the `lm()` function. You then get the model summary with the `summary()` function.
 
Alternatively, check [{modelsummary}](https://vincentarelbundock.github.io/modelsummary/) or [{stargazer}](https://www.jakeruss.com/cheatsheets/stargazer/) package to pretty-print the results of your regression model(s).
 
Define both dependent (y_train) and independent (X_train) variables and clean the data if necessary.
::::
 
 
 
:::: {.tipsp .python data-latex="p"}
For the next step the X_train values need to be reshaped .values.reshape(-1,1). Note: If you use more than one feature, you don’t have to reshape your data.
 
Import LinearRegression() from sklear.linear_model and train your model using LinearRegression().fit().
::::
 
 
Does your independent variable has a statistically significant impact on popularity? Probably yes, because we selected the variable most correlated with popularity. However, if we stick to this very simplified model, we make a typical mistake: the so-called Omitted Variable Bias (OVB). To put it simply, we neglect (in statistics jargon: “do not control for”) variables that have a significant influence on the dependent variable. One could suspect that other influencing factors also play an important role in estimating a song’s popularity. If we do not include them, the estimate of the effect of, e.g., danceability, is biased and hardly helpful. In our case, this is not a big problem for the time being since we are not interested in causal effects but rather in the best possible prediction. Your statistics professor would almost certainly object to such a model. Nonetheless, with just a single explanatory variable, this model will not necessarily predict the popularity well.
 
One possible solution is to include the omitted variables in the model. That is quite practical as the data set already includes these variables. So let’s set up a somewhat more extensive model that includes an additional variable:

$$ popularity = \beta_0 + \beta_1 \cdot danceability + \beta_2 \cdot energy + \epsilon  $$

Now compare the results of these two models. Does the larger model explain a higher proportion of the variance in popularity? In other words, which model shows the higher value for the $R^2$ measure?
 
:::: {.tips .r data-latex="r"}
Use the `summary()` function of your model to compare the two models. Which model shows a higher value for $R^2$? Then you can compare the RMSE between your two models.
::::
 
 
:::: {.tipsp .python data-latex="p"}
You can now compare your two models, therefore compute for each model the $R^2$ value.

```import sklearn.metrics as metrics 
r2=metrics.r2_score(y_true, y_pred)```
::::
 
## From Training to Testing – Making Predictions

You have now trained your first model with the training data set. But how well does the model handle data it has not seen yet? This out-of-sample testing is a critical test to evaluate the quality of your model.
 
Did your model only “memorize” the existing patterns in the training data set? Then the relationships from the training data set would not be transferable to the test data set. With so-called overfitting, you trained the model too closely to the training data set and therefore received poor predictions for unknown data – for example, the data in your test set.
 
On the other hand, underfitting is also a problem: Your model did not sufficiently learn the actual relationships between the data. Thus, it makes poor predictions on the test data set. So it is essential to find a balance between the two problems.
 
Now the distinction between training and test data sets becomes essential. As a reminder: we use the train data to train a model and the test data to test our model’s quality.
 
To test your model on previously unseen data, you can apply it to the test data set.
 
:::: {.tips .r data-latex="r"}
Use the predict function for this:
 
`predicted_popularity <- predict(your_saved_model, your_test_data_frame)`
 
You have now created a vector with all popularity predictions for the test data set. You can now compare this with the actual values for popularity from the test data set.
 
In order to use a uniform comparison metric, please use the following function to measure your prediction accuracy:
 
``` # Function that returns Root Mean Squared Error while ignoring NAs```

```rmse <- function(actual, predicted) {
sqrt(mean((predicted - actual)^2, na.rm = TRUE))}```

After training, import the data from the `test.csv` file, define both variables `X_test` and `y_test`, and create a vector with price predictions applying `predict(X_test)` on your model. Store your prediction in the variable `predicted_popularity`.
::::
 
:::: {.tipsp .python data-latex="p"}
Finally, compare your predicted values with the test values:
 
from sklearn.metrics import mean_squared_error
# Function that returns Root Mean Squared Error while ignoring NaNs
rmse = mean_squared_error(y_test, predicted_popularity, squared=False)
::::
 
Now compare both regression models. Does the larger model have better prediction accuracy, i.e., a lower  $RMSE$? Now you have a benchmark for your more advanced models, which you can beat in the next part.

## Apply Advanced Machine Learning Algorithms

Now that you have created and tested an initial prediction using a simple regression model, you can apply more advanced methods. The goal is to get the lowest possible $RMSE$ when applying the model to the test data set. Look for at least one other algorithm and check whether it gives you a more accurate prediction (expressed as a lower $RMSE$ ). You can find inspiration for this in the advanced DataCamp courses listed at the beginning of the project guide. There are no limits – you can refine the regression using specific methods (e.g., LASSO) or set up a random forest model or a neural network. It is usually a good idea to briefly recall the respective algorithm’s functionality and consider whether this methodology makes sense with a continuous prediction variable.
 
You can also get a noticeable gain in predictive power by modifying existing variables or generating new variables from the data set (“feature engineering”). For example, you could create features based on artists. You could count the number of songs each artist produced or the mean popularity of each artist. You could also write a function that computes each artist’s mean popularity in the train data set and adds these values to the train data set. Afterward, you can add the same values to the test set, but keep in mind that there can be artists in the test set who are not in the train set. Therefore you could set the values for these artists to the mean popularity of all artists.
 
Always compare the $RMSE$ of your advanced models with each other, as well as with the benchmark regression model from before.
 
Congratulations! You’ve made it to the end of your TechAcademy Data Science project. After visualizing the data in the first part, you’ve also set up predictive models in this section. If you stuck around until this part and managed to code the better part of the exercises, you’ve definitively earned your certificate! We hope you had fun learning Data Science with this data set and that you enjoyed it – at least the parts where you didn’t get stuck forever because of some unexplainable coding error. Don’t forget to send your project results to our project submission email address before the deadline.
